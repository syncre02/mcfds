{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 (Mathematical and Computational Foundations of Data Science)\n",
    "- Instructor: [Sina Hazratpour](https://sinhp.github.io)\n",
    "- Zulip server: [mcfds.zulipchat.com](https://mcfds.zulipchat.com)\n",
    "\n",
    "\n",
    "## Homework Policies\n",
    "Homework should be submitted through Gradescope. The code needed to access gradescope was sent via e-mail.\n",
    "\n",
    "All answers should be fully motivated, with logically complete arguments and motivations. No late homework will be accepted. \n",
    "\n",
    "All submitted homework must be written up individually without consulting anyone else’s written solution.\n",
    "\n",
    "The submission of homework that require coding needs to be fully commented, the inputs and and outputs (text and/or figures) clearly explained. Both the correctness of the code and the clarity of comments will contribute to the grade you receive. Often the commenting is more extensive than the code itself. You should discuss the outputs of your code, critically and as needed in order to answer specific questions in the problems, so that the questions are fully answered. Outputs without comments/discussion will not be counted.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 1 (QR Meets Least^2)\n",
    "\n",
    "Suppose $A$ is a _tall_ $n \\times d$ matrix, i.e. $n > d$. Consider the full QR factorization $A = Q R$ where $Q$ is an $n \\times n$ orthogonal matrix and $R$ is an $n \\times d$ upper-triangular matrix.\n",
    "\n",
    "1. Show that columns of $A$ are linearly independent if and only if all the diagonal entires of $R$ are non-zero. \n",
    "\n",
    "For the rest of this exercise (parts 2-7), we assume that the columns of $A$ are linearly independent.\n",
    "\n",
    "2. Show that the matrix $A^T A$ is invertible.  \n",
    "\n",
    "Suppose $b \\in \\mathbb{R}^n$. As we have seen in the lecture, the vector $\\hat{x} = (A^T A)^{-1} b$ has the property of making the vector $A\\hat{x}$ is closest to the vector $b$. \n",
    "\n",
    "3. Show that the vector $b - A \\hat{x}$ is orthogonal to the column space of $A$.    \n",
    "\n",
    "4. Prove that  $A \\hat{x} = Q \\, Q^T \\, b$, and conclude that the orthogonal projection of $b$ onto the column space of $A$ is $A\\hat{x}$.  \n",
    "\n",
    "5. Show that $ \\| A \\hat{x} − b \\|^2 = \\| b \\|^2 −\\|Q^T b\\|^2$ . \n",
    "   \n",
    "6. Consider the matrix $A = \\begin{bmatrix} 2 & 0 \\\\ -1 & 1 \\\\ 0 & 2 \\\\   \\end{bmatrix}$. First, find the QR factorization of $A$ by hand.  Then confirm it using Python.\n",
    "   \n",
    "7. Consider the vector $b = \\begin{bmatrix} 1  \\\\ 0 \\\\ -1 \\\\   \\end{bmatrix}$. Use part 5 to find $\\min \\|  A x − b \\|  \\|$ with respect to all $x \\in \\mathbb{R}^2$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (Ridge Regression)\n",
    "\n",
    "As before $A$ is am $n \\times d$ matrix.  The __Ridge regression__ adds a norm-2 _penalty_ term to the least square problem in order to obtain a solution of smaller norm, and minimizes\n",
    "$$\\min \\| Ax − y \\|^2 + λ \\| x\\|^2 $$ \n",
    "over all  $x \\in R^d$ for some penalization parameter $λ > 0$.\n",
    "\n",
    "\n",
    "1. Show that the unique solution to the minimization problem above is given by $x^{\\text Ridge}_\\lambda = (A^T A + λ I )^{−1} A^T b$ .\n",
    "\n",
    "A cool fact is that the matrix $(A^T A + λ I )^{−1} A^T$ tends to the pseudo-inverse $A^+$ of $A$ -- which we obtained using SVD -- as the parameter $\\lambda$ tends to $0$. \n",
    "\n",
    "2. Suppose A has linearly independent columns. Explain why part 1 above implies that $A^+ = (A^T A)^{−1}A^T$ . Conclude that $x^{\\text Ridge}_\\lambda$ tends to $\\hat{x}$ from Problem 1 as the parameter $\\lambda$ tends to $0$. . \n",
    "\n",
    "For part 2, You will need the fact that the matrix inversion is a continuous function, which means that the limit of the inverse of a matrix is the inverse of the limit, provided the limit matrix is invertible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, generate a random 20 × 10 matrix $A$ and a random 20-vector $b$. Compute the least squares approximate solution $x^+ = A^+b$ and the associated residual norm squared $\\| A x^+ − b \\|^2$. Generate four different random 10-vectors $u_1 , u_2 , u_3, u_4$ , and verify that $\\| A(x^{+} + u_i ) − b \\|^2 > \\| Ax^+ − b\\|^2$ holds. This shows that any affine perturbation of $x^+$ however small makes $A x$ further apart from $b$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7943809e827f2ce7da8c5b2d12a479f6c08cd6fd5cc5e4389af6fb02edc8fdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
