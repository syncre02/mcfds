{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 (Mathematical and Computational Foundations of Data Science)\n",
    "- Instructor: [Sina Hazratpour](https://sinhp.github.io)\n",
    "- Zulip server: [mcfds.zulipchat.com](https://mcfds.zulipchat.com)\n",
    "\n",
    "\n",
    "## Homework Policies\n",
    "Homework should be submitted through Gradescope. The code needed to access gradescope was sent via e-mail.\n",
    "\n",
    "All answers should be fully motivated, with logically complete arguments and motivations. No late homework will be accepted. \n",
    "\n",
    "All submitted homework must be written up individually without consulting anyone else’s written solution.\n",
    "\n",
    "The submission of homework that require coding needs to be fully commented, the inputs and and outputs (text and/or figures) clearly explained. Both the correctness of the code and the clarity of comments will contribute to the grade you receive. Often the commenting is more extensive than the code itself. You should discuss the outputs of your code, critically and as needed in order to answer specific questions in the problems, so that the questions are fully answered. Outputs without comments/discussion will not be counted.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 1 (QR Meets Least^2)\n",
    "\n",
    "Suppose $A$ is a _tall_ $n \\times d$ matrix, i.e. $n > d$. Consider the full QR factorization $A = Q R$ where $Q$ is an $n \\times n$ orthogonal matrix and $R$ is an $n \\times d$ upper-triangular matrix.\n",
    "\n",
    "1. Show that columns of $A$ are linearly independent if and only if all the diagonal entires of $R$ are non-zero. \n",
    "\n",
    "For the rest of this exercise (parts 2-7), we assume that the columns of $A$ are linearly independent.\n",
    "\n",
    "2. Show that the matrix $A^T A$ is invertible.  \n",
    "\n",
    "Suppose $b \\in \\mathbb{R}^n$. As we have seen in the lecture, the vector $\\hat{x} = (A^T A)^{-1} b$ has the property of making the vector $A\\hat{x}$ is closest to the vector $b$. \n",
    "\n",
    "3. Show that the vector $b - A \\hat{x}$ is orthogonal to the column space of $A$.    \n",
    "\n",
    "4. Prove that  $A \\hat{x} = Q \\, Q^T \\, b$, and conclude that the orthogonal projection of $b$ onto the column space of $A$ is $A\\hat{x}$.  \n",
    "\n",
    "5. Show that $ \\| A \\hat{x} − b \\|^2 = \\| b \\|^2 −\\|Q^T b\\|^2$ . \n",
    "   \n",
    "6. Consider the matrix $A = \\begin{bmatrix} 2 & 0 \\\\ -1 & 1 \\\\ 0 & 2 \\\\   \\end{bmatrix}$. First, find the QR factorization of $A$ by hand.  Then confirm it using Python.\n",
    "   \n",
    "7. Consider the vector $b = \\begin{bmatrix} 1  \\\\ 0 \\\\ -1 \\\\   \\end{bmatrix}$. Use part 5 to find $\\min \\|  A x − b \\|  \\|$ with respect to all $x \\in \\mathbb{R}^2$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "First, we prove that if all the diagonal entries of R are non-zero, then A is linearly indepedent.\n",
    "If R is upper-triangular and have all non-zero entries, then it is invertible. \n",
    "Now, suppose we have x that is part of the null space of A s.t. Ax = 0.\n",
    "Then this also means that Q^TAx = 0 = R^Tx\n",
    "This implies x = 0 since R^T has all diagonal entires that are non-zero. \n",
    "So, that means A has to be linearly indepdent.\n",
    "\n",
    "Now, Suppose the columns of A are linearly indepdent.\n",
    "Then suppose there exists a diagional entry that is zero in R -> r_kk\n",
    "Then, the kth column of A must be a linear combination of the other coloumns.\n",
    "But this is a contradiction since A has to be linearly indepdent.\n",
    "\n",
    "# Answer 2\n",
    "One way to prove that it's invertible is to prove that the nullspace is only the 0 vector.\n",
    "\n",
    "Suppose there is a x that exists s.t. A^TAv = 0.\n",
    "Then, it must be that v^TA^TAv = 0.\n",
    "Well, this is equal to the square of the norm of V, \n",
    "v^TA^TAv = |Av|^2\n",
    "But, we know that as A has linearly indepdent columns, Av is only equal to 0 when v is equal to 0. So, the the nullspace is only the 0 vector and A^TA is invertible.\n",
    "\n",
    "# Answer 3\n",
    "To show that's orthoonal, then we need to show that (b-Ax)^T * a = 0 where a is an arbitray column in A.\n",
    "\n",
    "(b-Ax)^T * a = (b^T - x^TA^T)a = (b^Ta - x^TA^Ta) = b^Ta - (A^TA)^(-1)A^Tb^TAa = b^Ta - b^TA(A^TA)^(-1)A^Ta = b^Ta - b^Ta = 0\n",
    "as b^TA(A^TA)^-1A^Ta = b^Ta because A(A^TA)^(-1)A^Ta is the orthogonal projection of a onto A which is just a.\n",
    "So, b - Ax is orthogonal to the column space of A.\n",
    "\n",
    "# Answer 4\n",
    "Ax = QQ^Tb\n",
    "Ax = A(A^TA)^(-1)b = QR(R^TQ^TQR)^(-1)b = Q(R^TQ^TQR)^(-1)R^TQ^Tb = QR(R^TR)^(-1)R^TQ^Tb = QRR^TQ^Tb = QQ^Tb\n",
    "\n",
    "R^T is a diagonal upper traingular matrix so R^T = R^T^(-1) and QQ^T is the orthogonal projection onto the column space of A. So, the orthogonal projection of b onto the column space of A is \n",
    "Ax = QQ^Tb\n",
    "\n",
    "# Answer 5\n",
    "$ \\| A \\hat{x} − b \\|^2 = \\| b \\|^2 −\\|Q^T b\\|^2$\n",
    "\n",
    "A \\hat{x} = A(A^TA)^(-1)b = Q(R^TQ^T)(QR)^(-1)b = QQ^(-1)R(R^T)^(-1)Q^Tb = Q^Tb\n",
    "\n",
    "So, Ax - Q^Tb = 0 so A^x - b is orthogonal to Q. So, |Q^Tb| = |b|\n",
    "\n",
    "$ \\| A \\hat{x} − b \\|^2 = |Ax - Q^Tb \\|^2 $ \n",
    "\n",
    "Foiling this out:\n",
    "\n",
    "|Ax|^2 - 2(Ax)^T(Q^Tb) + |Q^Tb|^2\n",
    "\n",
    "Now, we can just subtitute Ax for Q^Tb\n",
    "|Q^Tb|^2 - 2(Q^Tb)^T(Q^Tb) + |Q^Tb|^2\n",
    "\n",
    "As stated above, |Q^Tb| = |b|\n",
    "\n",
    "|b|^2 - 2|Q^Tb|^2 + |Q^Tb|^2 = |b|^2 - |Q^Tb|^2\n",
    "\n",
    "# Answer 6\n",
    "Using Grahm Schmidt:\n",
    "v1 = a1 = [2 -1 0]\n",
    "u1 = [2 -1 0]/||v1|| = [2 -1 0]/root(5) = [2 sqrt(5)/5 -1 sqrt(5)/5 0]\n",
    "\n",
    "u2 = v2 - proju1(v2) = [2/5 4/5 2]\n",
    "\n",
    "e2 = u2/|u2| = [sqrt(30)/30 sqrt(30)/15 sqrt(30)/6]\n",
    "\n",
    "So Q^T =  [2 sqrt(5)/5 -sqrt(5)/5 0],\n",
    "        [sqrt(30)/30 sqrt(30)/15 sqrt(30)/6]\n",
    "\n",
    "Q = 2 sqrt(5)/5 sqrt(30)/30\n",
    "    -sqrt(5)/5 sqrt(30)/15\n",
    "    0           sqrt(30)/6\n",
    "    \n",
    "R = [||V1|| <u1, a2>\n",
    "        0       ||v2|| ] \n",
    "        0          0\n",
    "\n",
    "R = [sqrt(5) -sqrt(5)5\n",
    "        0     2sqrt(30)/5\n",
    "        0       0 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0]\n",
      " [-1  1]\n",
      " [ 0  2]]\n",
      "[[-0.89442719 -0.18257419]\n",
      " [ 0.4472136  -0.36514837]\n",
      " [-0.         -0.91287093]]\n",
      "[[-2.23606798  0.4472136 ]\n",
      " [ 0.         -2.19089023]]\n"
     ]
    }
   ],
   "source": [
    "# Answer 6 in code\n",
    "import numpy as np\n",
    "A = np.array([[2, 0], [-50, 50], [0, 2]])\n",
    "Q, R = np.linalg.qr(A)\n",
    "print(A)\n",
    "print(Q)\n",
    "print(R)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 7\n",
    "\n",
    "Notice, from part 5 the only part that we can change is ||Q^Tb||^2 as ||b||^2 is the same given we know b.\n",
    "\n",
    "We want to maximize ||Q^Tb||^2 (or ||Q^Tb||) then.\n",
    "\n",
    "So we need to find what Q^T should be to minimize ||Ax-b|| or maximize ||Q^Tb||\n",
    "\n",
    "Well, given that A is the same for 6 and 7, Q^T it's already set for us\n",
    "Q^T =  [2 sqrt(5)/5 -sqrt(5)/5 0],\n",
    "        [sqrt(30)/30 sqrt(30)/15 sqrt(30)/6]\n",
    "\n",
    "So we just plug this in to the inner product to get the value.\n",
    "\n",
    "||Q^Tb||^2 = 2/5\n",
    "\n",
    "||b||^2 = 2\n",
    "\n",
    "min ||Ax -b|| = sqrt(||b||^2 - ||Q^Tb||^2) = sqrt(2 - 2/5) = sqrt(8/5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (Ridge Regression)\n",
    "\n",
    "As before $A$ is am $n \\times d$ matrix.  The __Ridge regression__ adds a norm-2 _penalty_ term to the least square problem in order to obtain a solution of smaller norm, and minimizes\n",
    "$$\\min \\| Ax − y \\|^2 + λ \\| x\\|^2 $$ \n",
    "over all  $x \\in R^d$ for some penalization parameter $λ > 0$.\n",
    "\n",
    "\n",
    "1. Show that the unique solution to the minimization problem above is given by $x^{\\text Ridge}_\\lambda = (A^T A + λ I )^{−1} A^T b$ .\n",
    "\n",
    "A cool fact is that the matrix $(A^T A + λ I )^{−1} A^T$ tends to the pseudo-inverse $A^+$ of $A$ -- which we obtained using SVD -- as the parameter $\\lambda$ tends to $0$. \n",
    "\n",
    "2. Suppose A has linearly independent columns. Explain why part 1 above implies that $A^+ = (A^T A)^{−1}A^T$ . Conclude that $x^{\\text Ridge}_\\lambda$ tends to $\\hat{x}$ from Problem 1 as the parameter $\\lambda$ tends to $0$. . \n",
    "\n",
    "For part 2, You will need the fact that the matrix inversion is a continuous function, which means that the limit of the inverse of a matrix is the inverse of the limit, provided the limit matrix is invertible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "To minimize the function, we need to take the derative of the function in regards to x.\n",
    "d/dx (||Ax - y||^2 + λ||x||^2) = 2A^T(Ax - y) + 2λx = 2(A^TA + λI)x - 2A^Ty = 0\n",
    "\n",
    "Now we solve for x to minimize the function\n",
    "x = (A^TA + λI)^(-1)A^Ty\n",
    "\n",
    "Now, this is a formula for the Critical Points but to show that it's a minimum, we need to solve the second derivative of the function in regards to x.\n",
    "\n",
    "d^2/dx^2 (||Ax - y||^2 + λ||x||^2) = 2(A^TA + λI)\n",
    "\n",
    "Since λ > 0 and A^TA is positive definite, the matrix (A^TA + λI) is positive definite. So, the second derivative is positive definite and the critical point is a minimum.\n",
    "\n",
    "# Answer 2\n",
    "\n",
    "If A has linearly independent columns, then A^T is invertible. It also has full rank. \n",
    "\n",
    "Also, A^TA is a square matrix which is also invertible since A has linearly independent columns.\n",
    "\n",
    "A^+ = (A^T A)^(-1)A^T as A^+ is the pseudo inverse of A.  \n",
    "\n",
    "Now taking the limit as λ -> 0, we get:\n",
    "\n",
    "lim λ->0 (A^T A + λI)^(-1)A^Tb = lim λ->0 (A^T A)^(-1)A^Tb\n",
    "\n",
    "Which is the same as the result from part 1.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, generate a random 20 × 10 matrix $A$ and a random 20-vector $b$. Compute the least squares approximate solution $x^+ = A^+b$ and the associated residual norm squared $\\| A x^+ − b \\|^2$. Generate four different random 10-vectors $u_1 , u_2 , u_3, u_4$ , and verify that $\\| A(x^{+} + u_i ) − b \\|^2 > \\| Ax^+ − b\\|^2$ holds. This shows that any affine perturbation of $x^+$ however small makes $A x$ further apart from $b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Generate the matrix and vector\n",
    "A = np.random.uniform(-50, 50, size=(20, 10))\n",
    "b = np.random.uniform(-50, 50, size=20)\n",
    "\n",
    "# Calculate the Least Squares Approximiation using A and b as well as the residual norm\n",
    "x_plus, residualNorm, rank, singularValues = np.linalg.lstsq(A, b, rcond = None )\n",
    "\n",
    "#Generate the 4 random 10-vectors\n",
    "u_1 = np.random.uniform(-50, 50, size=10)\n",
    "u_2 = np.random.uniform(-50, 50, size=10)\n",
    "u_3 = np.random.uniform(-50, 50, size=10)\n",
    "u_4 = np.random.uniform(-50, 50, size=10)\n",
    "\n",
    "#Calculate the norms for each\n",
    "norm_1 = np.linalg.norm(A.dot(x_plus + u_1) - b)**2\n",
    "norm_2 = np.linalg.norm(A.dot(x_plus + u_2) - b)**2\n",
    "norm_3 = np.linalg.norm(A.dot(x_plus + u_3) - b)**2\n",
    "norm_4 = np.linalg.norm(A.dot(x_plus + u_4) - b)**2\n",
    "\n",
    "#Calculate the norm of Ax^+ -b\n",
    "\n",
    "norm_x_plus = np.linalg.norm(A.dot(x_plus) - b)**2\n",
    "\n",
    "#Verify that the statement is true\n",
    "print(norm_1 > norm_x_plus)\n",
    "print(norm_2 > norm_x_plus)\n",
    "print(norm_3 > norm_x_plus)\n",
    "print(norm_4 > norm_x_plus)\n",
    "\n",
    "# It is true for all 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7943809e827f2ce7da8c5b2d12a479f6c08cd6fd5cc5e4389af6fb02edc8fdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
